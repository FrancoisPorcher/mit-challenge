{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Isaac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "import utils_isaac as utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# This is used to import the evaluation script, not needed for training\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    challenge_data_dir = Path('../../dataset/'),\n",
    "    valid_ratio = 0.1,\n",
    "    lag_steps = 6,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the list of feature columns\n",
    "feature_cols = [\n",
    "    \"Eccentricity\",\n",
    "    \"Semimajor Axis (m)\",\n",
    "    \"Inclination (deg)\",\n",
    "    \"RAAN (deg)\",\n",
    "    \"Argument of Periapsis (deg)\",\n",
    "    \"True Anomaly (deg)\",\n",
    "    \"Latitude (deg)\",\n",
    "    \"Longitude (deg)\",\n",
    "    \"Altitude (m)\",\n",
    "    \"X (m)\",\n",
    "    \"Y (m)\",\n",
    "    \"Z (m)\",\n",
    "    \"Vx (m/s)\",\n",
    "    \"Vy (m/s)\",\n",
    "    \"Vz (m/s)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = pd.read_csv(config.challenge_data_dir / 'train_labels.csv')\n",
    "\n",
    "# # Apply the function to the ground truth data\n",
    "# data, updated_feature_cols = utils.tabularize_data(train_data_dir,\n",
    "#                                                    feature_cols, \n",
    "#                                                    ground_truth,\n",
    "#                                                    lag_steps=config.lag_steps,\n",
    "#                                                    add_heurestic=False)\n",
    "\n",
    "# data['EW'] = data['EW'].fillna('Nothing')\n",
    "# data['NS'] = data['NS'].fillna('Nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data[['ObjectID','Timestamp','EW_baseline_heuristic','NS_baseline_heuristic','EW_baseline_heuristic_ffill','NS_baseline_heuristic_ffill']].to_pickle('Data_With_Baseline.pkl')\n",
    "#data.to_pickle('Data_With_Nothing.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_baseline = pd.read_pickle('Data_With_Baseline.pkl')\n",
    "data = pd.read_pickle('Data_With_Nothing.pkl')\n",
    "data = pd.merge(data,data_baseline,on=['ObjectID','Timestamp'],how='left')\n",
    "data = pd.concat([data,\n",
    "                  pd.get_dummies(data[['EW_baseline_heuristic']]),\n",
    "                  pd.get_dummies(data[['NS_baseline_heuristic']]),\n",
    "                  pd.get_dummies(data[['EW_baseline_heuristic_ffill']]),\n",
    "                  pd.get_dummies(data[['NS_baseline_heuristic_ffill']])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del data_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.loc[data.ObjectID.isin(list(data.ObjectID.unique())[:500])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rolling_features = []\n",
    "# for variable in [ \"Inclination (deg)\",  \"RAAN (deg)\", \"Argument of Periapsis (deg)\", \"True Anomaly (deg)\", \"Altitude (m)\",  \"Vx (m/s)\",  \"Vy (m/s)\", \"Vz (m/s)\"]:\n",
    "#     rolling_std_name = f'{variable}_rolling_std_12'\n",
    "#     rolling_features.append(data.groupby('ObjectID')[variable].apply(lambda win:win.rolling(12,center=True).std().ffill().bfill()).rename(rolling_std_name))\n",
    "#     new_feature_cols.append(rolling_std_name)\n",
    "#     rolling_mean_name = f'{variable}_rolling_mean_12'\n",
    "#     rolling_features.append(data.groupby('ObjectID')[variable].apply(lambda win:win.rolling(12,center=True).std().ffill().bfill()).rename(rolling_mean_name))\n",
    "#     new_feature_cols.append(rolling_std_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to_add = []\n",
    "# for factor in [ \"Altitude (m)\", \"Inclination (deg)\"]:\n",
    "#     charach_name = f'{factor}_charac'\n",
    "#     to_add.append(pd.merge(data[['ObjectID','Timestamp']],data.groupby('ObjectID')[factor].mean().rename(charach_name),on='ObjectID')[charach_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.concat([data] + rolling_features + to_add , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_feature_cols = list(data.columns)\n",
    "updated_feature_cols.remove('TimeIndex')\n",
    "updated_feature_cols.remove('Timestamp')\n",
    "updated_feature_cols.remove('ObjectID')\n",
    "updated_feature_cols.remove('EW')\n",
    "updated_feature_cols.remove('NS')\n",
    "updated_feature_cols.remove('EW_baseline_heuristic')\n",
    "updated_feature_cols.remove('NS_baseline_heuristic')\n",
    "updated_feature_cols.remove('EW_baseline_heuristic_ffill')\n",
    "updated_feature_cols.remove('NS_baseline_heuristic_ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(updated_feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a validation set without mixing the ObjectIDs\n",
    "object_ids = data['ObjectID'].unique()\n",
    "train_ids, valid_ids = train_test_split(object_ids, \n",
    "                                        test_size=config.valid_ratio, \n",
    "                                        random_state=43)\n",
    "\n",
    "train_data = data[data['ObjectID'].isin(train_ids)].copy()\n",
    "valid_data = data[data['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "ground_truth_train = ground_truth[ground_truth['ObjectID'].isin(train_ids)].copy()\n",
    "ground_truth_valid = ground_truth[ground_truth['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "# Count the number of objects in the training and validation sets\n",
    "print('Number of objects in the training set:', len(train_data['ObjectID'].unique()))\n",
    "print('Number of objects in the validation set:', len(valid_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make sure that there every label, both in the direction EW and NS,\n",
    "is present both in the training and validation partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the unique values of EW and NS in train and test data\n",
    "train_EW = set(train_data['EW'].unique())\n",
    "train_NS = set(train_data['NS'].unique())\n",
    "valid_EW = set(valid_data['EW'].unique())\n",
    "valid_NS = set(valid_data['NS'].unique())\n",
    "\n",
    "# Get the values of EW and NS that are in test data but not in train data\n",
    "missing_EW = valid_EW.difference(train_EW)\n",
    "missing_NS = valid_NS.difference(train_NS)\n",
    "\n",
    "# Check if all the values in EW are also present in NS\n",
    "if not set(train_data['EW'].unique()).issubset(set(train_data['NS'].unique())):\n",
    "    # Get the values of EW that are not present in NS\n",
    "    missing_EW_NS = set(train_data['EW'].unique()).difference(\n",
    "        set(train_data['NS'].unique())\n",
    "    )\n",
    "else:\n",
    "    missing_EW_NS = None\n",
    "\n",
    "# Print the missing values of EW and NS\n",
    "print(\"Missing values of EW in test data:\", missing_EW)\n",
    "print(\"Missing values of NS in test data:\", missing_NS)\n",
    "print(\"Values of EW not present in NS:\", missing_EW_NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert categorical data to numerical data\n",
    "le_EW = LabelEncoder()\n",
    "le_NS = LabelEncoder()\n",
    "\n",
    "# Encode the 'EW' and 'NS' columns\n",
    "train_data['EW_encoded'] = le_EW.fit_transform(train_data['EW'])\n",
    "train_data['NS_encoded'] = le_NS.fit_transform(train_data['NS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data['EW_baseline_heuristic_encoded'] = le_EW.transform(train_data['EW_baseline_heuristic'])\n",
    "# train_data['NS_baseline_heuristic_encoded'] = le_NS.transform(train_data['NS_baseline_heuristic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# valid_data['EW_baseline_heuristic_encoded'] = le_EW.transform(valid_data['EW_baseline_heuristic'])\n",
    "# valid_data['NS_baseline_heuristic_encoded'] = le_NS.transform(valid_data['NS_baseline_heuristic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Random Forest model for EW\n",
    "model_EW = CatBoostClassifier(n_estimators=100, random_state=42)\n",
    "# Fit the model to the training data for EW\n",
    "model_EW.fit(train_data[updated_feature_cols], train_data['EW_encoded'])\n",
    "\n",
    "# Define the Random Forest model for NS\n",
    "model_NS = CatBoostClassifier(n_estimators=100, random_state=42)\n",
    "# Fit the model to the training data for NS\n",
    "model_NS.fit(train_data[updated_feature_cols], train_data['NS_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_prediction(model,data,thresh):\n",
    "    pred_proba = pd.DataFrame(model.predict(data,prediction_type='Probability'))\n",
    "    pred = pred_proba.idxmax(1)\n",
    "    print('Num of ex to cut',sum(pred_proba.max(1)<thresh))\n",
    "    pred.loc[pred_proba.max(1)<thresh] = pred.value_counts().index[0]\n",
    "    pred = pred.to_numpy().reshape(-1,1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_ew = 0.1\n",
    "threshold_ns = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_EW = pickle.load(open('trained_model/model_EW.pkl', 'rb'))\n",
    "model_NS = pickle.load(open('trained_model/model_NS.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_0</th>\n",
       "      <td>4.297131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_-6</th>\n",
       "      <td>3.673737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_6</th>\n",
       "      <td>3.501534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_3</th>\n",
       "      <td>3.421403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-6</th>\n",
       "      <td>2.637408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_3</th>\n",
       "      <td>2.626182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-1</th>\n",
       "      <td>2.606135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_0</th>\n",
       "      <td>2.540590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_-6</th>\n",
       "      <td>2.490691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_1</th>\n",
       "      <td>2.430998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_2</th>\n",
       "      <td>2.355079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_2</th>\n",
       "      <td>2.327001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)</th>\n",
       "      <td>2.076461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_-5</th>\n",
       "      <td>2.030495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclination (deg)_lag_-4</th>\n",
       "      <td>2.022420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAN (deg)_lag_-6</th>\n",
       "      <td>1.688916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_-1</th>\n",
       "      <td>1.607253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_4</th>\n",
       "      <td>1.508169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altitude (m)_lag_-6</th>\n",
       "      <td>1.481956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_-2</th>\n",
       "      <td>1.461469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altitude (m)_lag_1</th>\n",
       "      <td>1.456878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altitude (m)_lag_2</th>\n",
       "      <td>1.346161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity</th>\n",
       "      <td>1.315985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_5</th>\n",
       "      <td>1.250522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_5</th>\n",
       "      <td>1.243675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_1</th>\n",
       "      <td>1.239382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_-5</th>\n",
       "      <td>1.198230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_-1</th>\n",
       "      <td>1.197036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_6</th>\n",
       "      <td>1.140022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altitude (m)_lag_-4</th>\n",
       "      <td>1.118813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-5</th>\n",
       "      <td>1.103975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altitude (m)_lag_6</th>\n",
       "      <td>1.101627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_1</th>\n",
       "      <td>1.060494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclination (deg)_lag_-5</th>\n",
       "      <td>1.040648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altitude (m)_lag_-5</th>\n",
       "      <td>1.017901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude (deg)_lag_2</th>\n",
       "      <td>1.017350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude (deg)_lag_-5</th>\n",
       "      <td>0.978808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude (deg)_lag_-2</th>\n",
       "      <td>0.918455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vx (m/s)_pct_change_-1</th>\n",
       "      <td>0.902131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclination (deg)_lag_2</th>\n",
       "      <td>0.892673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_6</th>\n",
       "      <td>0.836104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-2</th>\n",
       "      <td>0.804301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_0</th>\n",
       "      <td>0.775761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NS_baseline_heuristic_SS-NK</th>\n",
       "      <td>0.772910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude (deg)_lag_-6</th>\n",
       "      <td>0.738549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z (m)_lag_-4</th>\n",
       "      <td>0.738530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-4</th>\n",
       "      <td>0.719745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_-3</th>\n",
       "      <td>0.717424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)</th>\n",
       "      <td>0.695836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Argument of Periapsis (deg)</th>\n",
       "      <td>0.681761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "Eccentricity_lag_0           4.297131\n",
       "Semimajor Axis (m)_lag_-6    3.673737\n",
       "True Anomaly (deg)_lag_6     3.501534\n",
       "Eccentricity_lag_3           3.421403\n",
       "Eccentricity_lag_-6          2.637408\n",
       "True Anomaly (deg)_lag_3     2.626182\n",
       "Eccentricity_lag_-1          2.606135\n",
       "True Anomaly (deg)_lag_0     2.540590\n",
       "True Anomaly (deg)_lag_-6    2.490691\n",
       "Semimajor Axis (m)_lag_1     2.430998\n",
       "Eccentricity_lag_2           2.355079\n",
       "Semimajor Axis (m)_lag_2     2.327001\n",
       "True Anomaly (deg)           2.076461\n",
       "True Anomaly (deg)_lag_-5    2.030495\n",
       "Inclination (deg)_lag_-4     2.022420\n",
       "RAAN (deg)_lag_-6            1.688916\n",
       "True Anomaly (deg)_lag_-1    1.607253\n",
       "True Anomaly (deg)_lag_4     1.508169\n",
       "Altitude (m)_lag_-6          1.481956\n",
       "Semimajor Axis (m)_lag_-2    1.461469\n",
       "Altitude (m)_lag_1           1.456878\n",
       "Altitude (m)_lag_2           1.346161\n",
       "Eccentricity                 1.315985\n",
       "Semimajor Axis (m)_lag_5     1.250522\n",
       "True Anomaly (deg)_lag_5     1.243675\n",
       "Eccentricity_lag_1           1.239382\n",
       "Semimajor Axis (m)_lag_-5    1.198230\n",
       "Semimajor Axis (m)_lag_-1    1.197036\n",
       "Eccentricity_lag_6           1.140022\n",
       "Altitude (m)_lag_-4          1.118813\n",
       "Eccentricity_lag_-5          1.103975\n",
       "Altitude (m)_lag_6           1.101627\n",
       "True Anomaly (deg)_lag_1     1.060494\n",
       "Inclination (deg)_lag_-5     1.040648\n",
       "Altitude (m)_lag_-5          1.017901\n",
       "Latitude (deg)_lag_2         1.017350\n",
       "Latitude (deg)_lag_-5        0.978808\n",
       "Latitude (deg)_lag_-2        0.918455\n",
       "Vx (m/s)_pct_change_-1       0.902131\n",
       "Inclination (deg)_lag_2      0.892673\n",
       "Semimajor Axis (m)_lag_6     0.836104\n",
       "Eccentricity_lag_-2          0.804301\n",
       "Semimajor Axis (m)_lag_0     0.775761\n",
       "NS_baseline_heuristic_SS-NK  0.772910\n",
       "Latitude (deg)_lag_-6        0.738549\n",
       "Z (m)_lag_-4                 0.738530\n",
       "Eccentricity_lag_-4          0.719745\n",
       "Semimajor Axis (m)_lag_-3    0.717424\n",
       "Semimajor Axis (m)           0.695836\n",
       "Argument of Periapsis (deg)  0.681761"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_EW.feature_importances_,index=model_EW.feature_names_).sort_values(0,ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_0</th>\n",
       "      <td>3.847315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-1</th>\n",
       "      <td>3.685322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude (deg)_lag_-3</th>\n",
       "      <td>2.984368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity</th>\n",
       "      <td>2.561023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vy (m/s)_lag_-3</th>\n",
       "      <td>2.522724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-4</th>\n",
       "      <td>2.257173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_4</th>\n",
       "      <td>2.238182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vy (m/s)_pct_change_1</th>\n",
       "      <td>1.931695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude (deg)_lag_-2</th>\n",
       "      <td>1.894006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-5</th>\n",
       "      <td>1.873976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vx (m/s)_lag_2</th>\n",
       "      <td>1.706406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_1</th>\n",
       "      <td>1.527718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y (m)_lag_-1</th>\n",
       "      <td>1.470002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vx (m/s)_pct_change_-1</th>\n",
       "      <td>1.453835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_-3</th>\n",
       "      <td>1.447514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_-1</th>\n",
       "      <td>1.387889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z (m)_lag_-5</th>\n",
       "      <td>1.357956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclination (deg)_lag_-6</th>\n",
       "      <td>1.330888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_2</th>\n",
       "      <td>1.290799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAN (deg)_lag_1</th>\n",
       "      <td>1.267019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X (m)_lag_-3</th>\n",
       "      <td>1.232163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAN (deg)_lag_5</th>\n",
       "      <td>1.212043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EW_baseline_heuristic_SS-CK</th>\n",
       "      <td>1.171095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vz (m/s)_pct_change_-1</th>\n",
       "      <td>1.136917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-2</th>\n",
       "      <td>1.135791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z (m)_lag_-4</th>\n",
       "      <td>1.134731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude (deg)_lag_2</th>\n",
       "      <td>1.126783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vz (m/s)_lag_-3</th>\n",
       "      <td>1.117271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_-6</th>\n",
       "      <td>1.067312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_0</th>\n",
       "      <td>1.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NS_baseline_heuristic_SS-NK</th>\n",
       "      <td>1.024230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vx (m/s)_lag_-2</th>\n",
       "      <td>1.020569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vz (m/s)_pct_change_1</th>\n",
       "      <td>0.972146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclination (deg)_lag_6</th>\n",
       "      <td>0.957762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Anomaly (deg)_lag_-6</th>\n",
       "      <td>0.939029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eccentricity_lag_6</th>\n",
       "      <td>0.910838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vz (m/s)_lag_-6</th>\n",
       "      <td>0.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y (m)_lag_1</th>\n",
       "      <td>0.893140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altitude (m)_lag_-5</th>\n",
       "      <td>0.867188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Argument of Periapsis (deg)_lag_-5</th>\n",
       "      <td>0.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y (m)_lag_-6</th>\n",
       "      <td>0.812513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vz (m/s)_lag_-4</th>\n",
       "      <td>0.811554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vx (m/s)_pct_change_1</th>\n",
       "      <td>0.799271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vy (m/s)_pct_change_-1</th>\n",
       "      <td>0.797387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NS_baseline_heuristic_ffill_IK-CK</th>\n",
       "      <td>0.755111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vy (m/s)_lag_0</th>\n",
       "      <td>0.742831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vx (m/s)_lag_-1</th>\n",
       "      <td>0.729867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude (deg)_lag_4</th>\n",
       "      <td>0.720297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAN (deg)_lag_4</th>\n",
       "      <td>0.704838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semimajor Axis (m)_lag_5</th>\n",
       "      <td>0.687180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0\n",
       "Eccentricity_lag_0                  3.847315\n",
       "Eccentricity_lag_-1                 3.685322\n",
       "Latitude (deg)_lag_-3               2.984368\n",
       "Eccentricity                        2.561023\n",
       "Vy (m/s)_lag_-3                     2.522724\n",
       "Eccentricity_lag_-4                 2.257173\n",
       "Eccentricity_lag_4                  2.238182\n",
       "Vy (m/s)_pct_change_1               1.931695\n",
       "Latitude (deg)_lag_-2               1.894006\n",
       "Eccentricity_lag_-5                 1.873976\n",
       "Vx (m/s)_lag_2                      1.706406\n",
       "Semimajor Axis (m)_lag_1            1.527718\n",
       "Y (m)_lag_-1                        1.470002\n",
       "Vx (m/s)_pct_change_-1              1.453835\n",
       "Semimajor Axis (m)_lag_-3           1.447514\n",
       "Semimajor Axis (m)_lag_-1           1.387889\n",
       "Z (m)_lag_-5                        1.357956\n",
       "Inclination (deg)_lag_-6            1.330888\n",
       "Semimajor Axis (m)_lag_2            1.290799\n",
       "RAAN (deg)_lag_1                    1.267019\n",
       "X (m)_lag_-3                        1.232163\n",
       "RAAN (deg)_lag_5                    1.212043\n",
       "EW_baseline_heuristic_SS-CK         1.171095\n",
       "Vz (m/s)_pct_change_-1              1.136917\n",
       "Eccentricity_lag_-2                 1.135791\n",
       "Z (m)_lag_-4                        1.134731\n",
       "Longitude (deg)_lag_2               1.126783\n",
       "Vz (m/s)_lag_-3                     1.117271\n",
       "Eccentricity_lag_-6                 1.067312\n",
       "Semimajor Axis (m)_lag_0            1.035000\n",
       "NS_baseline_heuristic_SS-NK         1.024230\n",
       "Vx (m/s)_lag_-2                     1.020569\n",
       "Vz (m/s)_pct_change_1               0.972146\n",
       "Inclination (deg)_lag_6             0.957762\n",
       "True Anomaly (deg)_lag_-6           0.939029\n",
       "Eccentricity_lag_6                  0.910838\n",
       "Vz (m/s)_lag_-6                     0.893900\n",
       "Y (m)_lag_1                         0.893140\n",
       "Altitude (m)_lag_-5                 0.867188\n",
       "Argument of Periapsis (deg)_lag_-5  0.845000\n",
       "Y (m)_lag_-6                        0.812513\n",
       "Vz (m/s)_lag_-4                     0.811554\n",
       "Vx (m/s)_pct_change_1               0.799271\n",
       "Vy (m/s)_pct_change_-1              0.797387\n",
       "NS_baseline_heuristic_ffill_IK-CK   0.755111\n",
       "Vy (m/s)_lag_0                      0.742831\n",
       "Vx (m/s)_lag_-1                     0.729867\n",
       "Latitude (deg)_lag_4                0.720297\n",
       "RAAN (deg)_lag_4                    0.704838\n",
       "Semimajor Axis (m)_lag_5            0.687180"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_NS.feature_importances_,index=model_NS.feature_names_).sort_values(0,ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on the training data for EW\n",
    "train_data['Predicted_EW'] = le_EW.inverse_transform(\n",
    "    #model_EW.predict(train_data[updated_feature_cols])\n",
    "    do_prediction(model_EW,train_data[updated_feature_cols],threshold_ew)\n",
    ")\n",
    "\n",
    "# Make predictions on the validation data for NS\n",
    "train_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "    #model_NS.predict(train_data[updated_feature_cols])\n",
    "    do_prediction(model_NS,train_data[updated_feature_cols],threshold_ns)\n",
    ")\n",
    "\n",
    "train_data['Predicted_EW'] = train_data['Predicted_EW'].mask(train_data['Predicted_EW']=='Nothing').ffill()\n",
    "train_data['Predicted_NS'] = train_data['Predicted_NS'].mask(train_data['Predicted_NS']=='Nothing').ffill() \n",
    "\n",
    "# # Print the first few rows of the test data with predictions for both EW and NS\n",
    "# train_data[['TimeIndex', 'ObjectID', 'EW', \n",
    "#             'Predicted_EW', 'NS', 'Predicted_NS']].groupby('ObjectID').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_results = utils.convert_classifier_output(train_data)\n",
    "train_results.loc[train_results.TimeIndex==0,'Node'] = 'SS'\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, train_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_ew = 0.1\n",
    "threshold_ns = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.valid_ratio > 0:\n",
    "    # Make predictions on the validation data for EW\n",
    "    valid_data['Predicted_EW'] = le_EW.inverse_transform(\n",
    "        #model_EW.predict(valid_data[updated_feature_cols])\n",
    "        do_prediction(model_EW,valid_data[updated_feature_cols],threshold_ew)\n",
    "    )\n",
    "\n",
    "    # Make predictions on the validation data for NS\n",
    "    valid_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "        #model_NS.predict(valid_data[updated_feature_cols])\n",
    "        do_prediction(model_NS,valid_data[updated_feature_cols],threshold_ns)\n",
    "    )\n",
    "    \n",
    "    valid_data['Predicted_EW'] = valid_data['Predicted_EW'].mask(valid_data['Predicted_EW']=='Nothing').ffill()\n",
    "    valid_data['Predicted_NS'] = valid_data['Predicted_NS'].mask(valid_data['Predicted_NS']=='Nothing').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NodeDetectionEvaluator` class in the evaluation module allows not only to\n",
    "compute the general score for a given dataset, but get evaluations per object, and\n",
    "even plots that show how the predictions look like in a timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.valid_ratio > 0:\n",
    "    valid_results = utils.convert_classifier_output(valid_data)\n",
    "    valid_results.loc[valid_results.TimeIndex==0,'Node'] = 'SS'\n",
    "\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  valid_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the validation set: {precision:.2f}')\n",
    "print(f'Recall for the validation set: {recall:.2f}')\n",
    "print(f'F2 for the validation set: {f2:.2f}')\n",
    "print(f'RMSE for the validation set: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "#evaluator.plot(np.random.choice(train_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over the Object IDs in the training set and call the evaluation\n",
    "# function for each object and aggregate the results\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "for oid in train_data['ObjectID'].unique():\n",
    "    tp, fp, fn, gt_object, p_object = evaluator.evaluate(oid)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "print(f'Total true positives: {total_tp}')\n",
    "print(f'Total false positives: {total_fp}')\n",
    "print(f'Total false negatives: {total_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "evaluator.plot(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained random forest models (and label encoders) to disk\n",
    "# Create the folder trained_model if it doesn't exist\n",
    "Path('trained_model').mkdir(exist_ok=True)\n",
    "pickle.dump(model_EW, open('trained_model/model_EW.pkl', 'wb'))\n",
    "pickle.dump(model_NS, open('trained_model/model_NS.pkl', 'wb'))\n",
    "pickle.dump(le_EW, open('trained_model/le_EW.pkl', 'wb'))\n",
    "pickle.dump(le_NS, open('trained_model/le_NS.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
