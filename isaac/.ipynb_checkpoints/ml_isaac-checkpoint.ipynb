{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Isaac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastcore.basics import Path, AttrDict\n",
    "import utils_isaac as utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# This is used to import the evaluation script, not needed for training\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    challenge_data_dir = Path('../../dataset/'),\n",
    "    valid_ratio = 0.1,\n",
    "    lag_steps = 6,\n",
    "    tolerance= 6, # Default evaluation tolerance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the list of feature columns\n",
    "feature_cols = [\n",
    "    \"Eccentricity\",\n",
    "    \"Semimajor Axis (m)\",\n",
    "    \"Inclination (deg)\",\n",
    "    \"RAAN (deg)\",\n",
    "    \"Argument of Periapsis (deg)\",\n",
    "    \"True Anomaly (deg)\",\n",
    "    \"Latitude (deg)\",\n",
    "    \"Longitude (deg)\",\n",
    "    \"Altitude (m)\",\n",
    "    \"X (m)\",\n",
    "    \"Y (m)\",\n",
    "    \"Z (m)\",\n",
    "    \"Vx (m/s)\",\n",
    "    \"Vy (m/s)\",\n",
    "    \"Vz (m/s)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:74: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[factor].apply(lambda win: win.rolling(3*12, center=True, min_periods=0).max()) - \\\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:75: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  merged_data.groupby('ObjectID')[factor].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:74: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[factor].apply(lambda win: win.rolling(3*12, center=True, min_periods=0).max()) - \\\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:75: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  merged_data.groupby('ObjectID')[factor].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:74: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[factor].apply(lambda win: win.rolling(3*12, center=True, min_periods=0).max()) - \\\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:75: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  merged_data.groupby('ObjectID')[factor].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:120: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n",
      "C:\\Users\\isaac\\Documents\\Challenge_Francois\\splid-devkit\\baseline_submissions\\isaac_ml\\utils_isaac.py:126: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  added_data = merged_data.groupby('ObjectID')[variable].apply(\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths\n",
    "train_data_dir = config.challenge_data_dir / \"train\"\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truth = pd.read_csv(config.challenge_data_dir / 'train_labels.csv')\n",
    "\n",
    "# # Apply the function to the ground truth data\n",
    "data, updated_feature_cols = utils.tabularize_data(train_data_dir,\n",
    "                                                   feature_cols, \n",
    "                                                   ground_truth,\n",
    "                                                   lag_steps=config.lag_steps,\n",
    "                                                   add_heurestic=False,\n",
    "                                                   nb_of_ex = 500)\n",
    "\n",
    "data['EW'] = data['EW'].fillna('Nothing')\n",
    "data['NS'] = data['NS'].fillna('Nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data[['ObjectID','Timestamp','EW_baseline_heuristic','NS_baseline_heuristic','EW_baseline_heuristic_ffill','NS_baseline_heuristic_ffill']].to_pickle('Data_With_Baseline.pkl')\n",
    "#data.to_pickle('Data_With_Nothing.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_baseline = pd.read_pickle('Data_With_Baseline.pkl')\n",
    "#data = pd.read_pickle('Data_With_Nothing.pkl')\n",
    "data = pd.merge(data,data_baseline,on=['ObjectID','Timestamp'],how='left')\n",
    "data = pd.concat([data,\n",
    "                  pd.get_dummies(data[['EW_baseline_heuristic']]),\n",
    "                  pd.get_dummies(data[['NS_baseline_heuristic']]),\n",
    "                  pd.get_dummies(data[['EW_baseline_heuristic_ffill']]),\n",
    "                  pd.get_dummies(data[['NS_baseline_heuristic_ffill']])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del data_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data = data.loc[data.ObjectID.isin(list(data.ObjectID.unique())[:500])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_feature_cols = list(data.columns)\n",
    "updated_feature_cols.remove('TimeIndex')\n",
    "updated_feature_cols.remove('Timestamp')\n",
    "updated_feature_cols.remove('ObjectID')\n",
    "updated_feature_cols.remove('EW')\n",
    "updated_feature_cols.remove('NS')\n",
    "updated_feature_cols.remove('EW_baseline_heuristic')\n",
    "updated_feature_cols.remove('NS_baseline_heuristic')\n",
    "updated_feature_cols.remove('EW_baseline_heuristic_ffill')\n",
    "updated_feature_cols.remove('NS_baseline_heuristic_ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eccentricity',\n",
       " 'Semimajor Axis (m)',\n",
       " 'Inclination (deg)',\n",
       " 'RAAN (deg)',\n",
       " 'Argument of Periapsis (deg)',\n",
       " 'True Anomaly (deg)',\n",
       " 'Latitude (deg)',\n",
       " 'Longitude (deg)',\n",
       " 'Altitude (m)',\n",
       " 'X (m)',\n",
       " 'Y (m)',\n",
       " 'Z (m)',\n",
       " 'Vx (m/s)',\n",
       " 'Vy (m/s)',\n",
       " 'Vz (m/s)',\n",
       " 'Latitude (deg)_enveloppe',\n",
       " 'Longitude (deg)_enveloppe',\n",
       " 'Altitude (m)_enveloppe',\n",
       " 'Eccentricity_lag_-6',\n",
       " 'Eccentricity_lag_-5',\n",
       " 'Eccentricity_lag_-4',\n",
       " 'Eccentricity_lag_-3',\n",
       " 'Eccentricity_lag_-2',\n",
       " 'Eccentricity_lag_-1',\n",
       " 'Eccentricity_lag_1',\n",
       " 'Eccentricity_lag_2',\n",
       " 'Eccentricity_lag_3',\n",
       " 'Eccentricity_lag_4',\n",
       " 'Eccentricity_lag_5',\n",
       " 'Eccentricity_lag_6',\n",
       " 'Semimajor Axis (m)_lag_-6',\n",
       " 'Semimajor Axis (m)_lag_-5',\n",
       " 'Semimajor Axis (m)_lag_-4',\n",
       " 'Semimajor Axis (m)_lag_-3',\n",
       " 'Semimajor Axis (m)_lag_-2',\n",
       " 'Semimajor Axis (m)_lag_-1',\n",
       " 'Semimajor Axis (m)_lag_1',\n",
       " 'Semimajor Axis (m)_lag_2',\n",
       " 'Semimajor Axis (m)_lag_3',\n",
       " 'Semimajor Axis (m)_lag_4',\n",
       " 'Semimajor Axis (m)_lag_5',\n",
       " 'Semimajor Axis (m)_lag_6',\n",
       " 'Inclination (deg)_lag_-6',\n",
       " 'Inclination (deg)_lag_-5',\n",
       " 'Inclination (deg)_lag_-4',\n",
       " 'Inclination (deg)_lag_-3',\n",
       " 'Inclination (deg)_lag_-2',\n",
       " 'Inclination (deg)_lag_-1',\n",
       " 'Inclination (deg)_lag_1',\n",
       " 'Inclination (deg)_lag_2',\n",
       " 'Inclination (deg)_lag_3',\n",
       " 'Inclination (deg)_lag_4',\n",
       " 'Inclination (deg)_lag_5',\n",
       " 'Inclination (deg)_lag_6',\n",
       " 'RAAN (deg)_lag_-6',\n",
       " 'RAAN (deg)_lag_-5',\n",
       " 'RAAN (deg)_lag_-4',\n",
       " 'RAAN (deg)_lag_-3',\n",
       " 'RAAN (deg)_lag_-2',\n",
       " 'RAAN (deg)_lag_-1',\n",
       " 'RAAN (deg)_lag_1',\n",
       " 'RAAN (deg)_lag_2',\n",
       " 'RAAN (deg)_lag_3',\n",
       " 'RAAN (deg)_lag_4',\n",
       " 'RAAN (deg)_lag_5',\n",
       " 'RAAN (deg)_lag_6',\n",
       " 'Argument of Periapsis (deg)_lag_-6',\n",
       " 'Argument of Periapsis (deg)_lag_-5',\n",
       " 'Argument of Periapsis (deg)_lag_-4',\n",
       " 'Argument of Periapsis (deg)_lag_-3',\n",
       " 'Argument of Periapsis (deg)_lag_-2',\n",
       " 'Argument of Periapsis (deg)_lag_-1',\n",
       " 'Argument of Periapsis (deg)_lag_1',\n",
       " 'Argument of Periapsis (deg)_lag_2',\n",
       " 'Argument of Periapsis (deg)_lag_3',\n",
       " 'Argument of Periapsis (deg)_lag_4',\n",
       " 'Argument of Periapsis (deg)_lag_5',\n",
       " 'Argument of Periapsis (deg)_lag_6',\n",
       " 'True Anomaly (deg)_lag_-6',\n",
       " 'True Anomaly (deg)_lag_-5',\n",
       " 'True Anomaly (deg)_lag_-4',\n",
       " 'True Anomaly (deg)_lag_-3',\n",
       " 'True Anomaly (deg)_lag_-2',\n",
       " 'True Anomaly (deg)_lag_-1',\n",
       " 'True Anomaly (deg)_lag_1',\n",
       " 'True Anomaly (deg)_lag_2',\n",
       " 'True Anomaly (deg)_lag_3',\n",
       " 'True Anomaly (deg)_lag_4',\n",
       " 'True Anomaly (deg)_lag_5',\n",
       " 'True Anomaly (deg)_lag_6',\n",
       " 'Latitude (deg)_lag_-6',\n",
       " 'Latitude (deg)_lag_-5',\n",
       " 'Latitude (deg)_lag_-4',\n",
       " 'Latitude (deg)_lag_-3',\n",
       " 'Latitude (deg)_lag_-2',\n",
       " 'Latitude (deg)_lag_-1',\n",
       " 'Latitude (deg)_lag_1',\n",
       " 'Latitude (deg)_lag_2',\n",
       " 'Latitude (deg)_lag_3',\n",
       " 'Latitude (deg)_lag_4',\n",
       " 'Latitude (deg)_lag_5',\n",
       " 'Latitude (deg)_lag_6',\n",
       " 'Longitude (deg)_lag_-6',\n",
       " 'Longitude (deg)_lag_-5',\n",
       " 'Longitude (deg)_lag_-4',\n",
       " 'Longitude (deg)_lag_-3',\n",
       " 'Longitude (deg)_lag_-2',\n",
       " 'Longitude (deg)_lag_-1',\n",
       " 'Longitude (deg)_lag_1',\n",
       " 'Longitude (deg)_lag_2',\n",
       " 'Longitude (deg)_lag_3',\n",
       " 'Longitude (deg)_lag_4',\n",
       " 'Longitude (deg)_lag_5',\n",
       " 'Longitude (deg)_lag_6',\n",
       " 'Altitude (m)_lag_-6',\n",
       " 'Altitude (m)_lag_-5',\n",
       " 'Altitude (m)_lag_-4',\n",
       " 'Altitude (m)_lag_-3',\n",
       " 'Altitude (m)_lag_-2',\n",
       " 'Altitude (m)_lag_-1',\n",
       " 'Altitude (m)_lag_1',\n",
       " 'Altitude (m)_lag_2',\n",
       " 'Altitude (m)_lag_3',\n",
       " 'Altitude (m)_lag_4',\n",
       " 'Altitude (m)_lag_5',\n",
       " 'Altitude (m)_lag_6',\n",
       " 'X (m)_lag_-6',\n",
       " 'X (m)_lag_-5',\n",
       " 'X (m)_lag_-4',\n",
       " 'X (m)_lag_-3',\n",
       " 'X (m)_lag_-2',\n",
       " 'X (m)_lag_-1',\n",
       " 'X (m)_lag_1',\n",
       " 'X (m)_lag_2',\n",
       " 'X (m)_lag_3',\n",
       " 'X (m)_lag_4',\n",
       " 'X (m)_lag_5',\n",
       " 'X (m)_lag_6',\n",
       " 'Y (m)_lag_-6',\n",
       " 'Y (m)_lag_-5',\n",
       " 'Y (m)_lag_-4',\n",
       " 'Y (m)_lag_-3',\n",
       " 'Y (m)_lag_-2',\n",
       " 'Y (m)_lag_-1',\n",
       " 'Y (m)_lag_1',\n",
       " 'Y (m)_lag_2',\n",
       " 'Y (m)_lag_3',\n",
       " 'Y (m)_lag_4',\n",
       " 'Y (m)_lag_5',\n",
       " 'Y (m)_lag_6',\n",
       " 'Z (m)_lag_-6',\n",
       " 'Z (m)_lag_-5',\n",
       " 'Z (m)_lag_-4',\n",
       " 'Z (m)_lag_-3',\n",
       " 'Z (m)_lag_-2',\n",
       " 'Z (m)_lag_-1',\n",
       " 'Z (m)_lag_1',\n",
       " 'Z (m)_lag_2',\n",
       " 'Z (m)_lag_3',\n",
       " 'Z (m)_lag_4',\n",
       " 'Z (m)_lag_5',\n",
       " 'Z (m)_lag_6',\n",
       " 'Vx (m/s)_lag_-6',\n",
       " 'Vx (m/s)_lag_-5',\n",
       " 'Vx (m/s)_lag_-4',\n",
       " 'Vx (m/s)_lag_-3',\n",
       " 'Vx (m/s)_lag_-2',\n",
       " 'Vx (m/s)_lag_-1',\n",
       " 'Vx (m/s)_lag_1',\n",
       " 'Vx (m/s)_lag_2',\n",
       " 'Vx (m/s)_lag_3',\n",
       " 'Vx (m/s)_lag_4',\n",
       " 'Vx (m/s)_lag_5',\n",
       " 'Vx (m/s)_lag_6',\n",
       " 'Vy (m/s)_lag_-6',\n",
       " 'Vy (m/s)_lag_-5',\n",
       " 'Vy (m/s)_lag_-4',\n",
       " 'Vy (m/s)_lag_-3',\n",
       " 'Vy (m/s)_lag_-2',\n",
       " 'Vy (m/s)_lag_-1',\n",
       " 'Vy (m/s)_lag_1',\n",
       " 'Vy (m/s)_lag_2',\n",
       " 'Vy (m/s)_lag_3',\n",
       " 'Vy (m/s)_lag_4',\n",
       " 'Vy (m/s)_lag_5',\n",
       " 'Vy (m/s)_lag_6',\n",
       " 'Vz (m/s)_lag_-6',\n",
       " 'Vz (m/s)_lag_-5',\n",
       " 'Vz (m/s)_lag_-4',\n",
       " 'Vz (m/s)_lag_-3',\n",
       " 'Vz (m/s)_lag_-2',\n",
       " 'Vz (m/s)_lag_-1',\n",
       " 'Vz (m/s)_lag_1',\n",
       " 'Vz (m/s)_lag_2',\n",
       " 'Vz (m/s)_lag_3',\n",
       " 'Vz (m/s)_lag_4',\n",
       " 'Vz (m/s)_lag_5',\n",
       " 'Vz (m/s)_lag_6',\n",
       " 'Eccentricity_diff_-2',\n",
       " 'Eccentricity_diff_-1',\n",
       " 'Eccentricity_diff_1',\n",
       " 'Eccentricity_diff_2',\n",
       " 'Semimajor Axis (m)_diff_-2',\n",
       " 'Semimajor Axis (m)_diff_-1',\n",
       " 'Semimajor Axis (m)_diff_1',\n",
       " 'Semimajor Axis (m)_diff_2',\n",
       " 'Inclination (deg)_diff_-2',\n",
       " 'Inclination (deg)_diff_-1',\n",
       " 'Inclination (deg)_diff_1',\n",
       " 'Inclination (deg)_diff_2',\n",
       " 'RAAN (deg)_diff_-2',\n",
       " 'RAAN (deg)_diff_-1',\n",
       " 'RAAN (deg)_diff_1',\n",
       " 'RAAN (deg)_diff_2',\n",
       " 'Argument of Periapsis (deg)_diff_-2',\n",
       " 'Argument of Periapsis (deg)_diff_-1',\n",
       " 'Argument of Periapsis (deg)_diff_1',\n",
       " 'Argument of Periapsis (deg)_diff_2',\n",
       " 'True Anomaly (deg)_diff_-2',\n",
       " 'True Anomaly (deg)_diff_-1',\n",
       " 'True Anomaly (deg)_diff_1',\n",
       " 'True Anomaly (deg)_diff_2',\n",
       " 'Latitude (deg)_diff_-2',\n",
       " 'Latitude (deg)_diff_-1',\n",
       " 'Latitude (deg)_diff_1',\n",
       " 'Latitude (deg)_diff_2',\n",
       " 'Longitude (deg)_diff_-2',\n",
       " 'Longitude (deg)_diff_-1',\n",
       " 'Longitude (deg)_diff_1',\n",
       " 'Longitude (deg)_diff_2',\n",
       " 'Altitude (m)_diff_-2',\n",
       " 'Altitude (m)_diff_-1',\n",
       " 'Altitude (m)_diff_1',\n",
       " 'Altitude (m)_diff_2',\n",
       " 'Latitude (deg)_enveloppe_diff_-2',\n",
       " 'Latitude (deg)_enveloppe_diff_-1',\n",
       " 'Latitude (deg)_enveloppe_diff_1',\n",
       " 'Latitude (deg)_enveloppe_diff_2',\n",
       " 'Longitude (deg)_enveloppe_diff_-2',\n",
       " 'Longitude (deg)_enveloppe_diff_-1',\n",
       " 'Longitude (deg)_enveloppe_diff_1',\n",
       " 'Longitude (deg)_enveloppe_diff_2',\n",
       " 'Altitude (m)_enveloppe_diff_-2',\n",
       " 'Altitude (m)_enveloppe_diff_-1',\n",
       " 'Altitude (m)_enveloppe_diff_1',\n",
       " 'Altitude (m)_enveloppe_diff_2',\n",
       " 'Vx (m/s)_pct_change_-1',\n",
       " 'Vx (m/s)_pct_change_1',\n",
       " 'Vy (m/s)_pct_change_-1',\n",
       " 'Vy (m/s)_pct_change_1',\n",
       " 'Vz (m/s)_pct_change_-1',\n",
       " 'Vz (m/s)_pct_change_1',\n",
       " 'Eccentricity_rolling_std_12',\n",
       " 'Eccentricity_rolling_mean_12',\n",
       " 'Semimajor Axis (m)_rolling_std_12',\n",
       " 'Semimajor Axis (m)_rolling_mean_12',\n",
       " 'Inclination (deg)_rolling_std_12',\n",
       " 'Inclination (deg)_rolling_mean_12',\n",
       " 'RAAN (deg)_rolling_std_12',\n",
       " 'RAAN (deg)_rolling_mean_12',\n",
       " 'Argument of Periapsis (deg)_rolling_std_12',\n",
       " 'Argument of Periapsis (deg)_rolling_mean_12',\n",
       " 'True Anomaly (deg)_rolling_std_12',\n",
       " 'True Anomaly (deg)_rolling_mean_12',\n",
       " 'Altitude (m)_rolling_std_12',\n",
       " 'Altitude (m)_rolling_mean_12',\n",
       " 'Vx (m/s)_rolling_std_12',\n",
       " 'Vx (m/s)_rolling_mean_12',\n",
       " 'Vy (m/s)_rolling_std_12',\n",
       " 'Vy (m/s)_rolling_mean_12',\n",
       " 'Vz (m/s)_rolling_std_12',\n",
       " 'Vz (m/s)_rolling_mean_12',\n",
       " 'Eccentricity_mean_charac',\n",
       " 'Semimajor Axis (m)_mean_charac',\n",
       " 'Inclination (deg)_mean_charac',\n",
       " 'RAAN (deg)_mean_charac',\n",
       " 'Argument of Periapsis (deg)_mean_charac',\n",
       " 'True Anomaly (deg)_mean_charac',\n",
       " 'Altitude (m)_mean_charac',\n",
       " 'X (m)_std_charac',\n",
       " 'Y (m)_std_charac',\n",
       " 'Z (m)_std_charac',\n",
       " 'Vx (m/s)_std_charac',\n",
       " 'Vy (m/s)_std_charac',\n",
       " 'Vz (m/s)_std_charac',\n",
       " 'EW_baseline_heuristic_AD-NK',\n",
       " 'EW_baseline_heuristic_ID-NK',\n",
       " 'EW_baseline_heuristic_IK-CK',\n",
       " 'EW_baseline_heuristic_IK-EK',\n",
       " 'EW_baseline_heuristic_Nothing',\n",
       " 'EW_baseline_heuristic_SS-CK',\n",
       " 'EW_baseline_heuristic_SS-EK',\n",
       " 'EW_baseline_heuristic_SS-NK',\n",
       " 'NS_baseline_heuristic_ID-NK',\n",
       " 'NS_baseline_heuristic_IK-CK',\n",
       " 'NS_baseline_heuristic_IK-EK',\n",
       " 'NS_baseline_heuristic_Nothing',\n",
       " 'NS_baseline_heuristic_SS-CK',\n",
       " 'NS_baseline_heuristic_SS-EK',\n",
       " 'NS_baseline_heuristic_SS-NK',\n",
       " 'EW_baseline_heuristic_ffill_AD-NK',\n",
       " 'EW_baseline_heuristic_ffill_ID-NK',\n",
       " 'EW_baseline_heuristic_ffill_IK-CK',\n",
       " 'EW_baseline_heuristic_ffill_IK-EK',\n",
       " 'EW_baseline_heuristic_ffill_SS-CK',\n",
       " 'EW_baseline_heuristic_ffill_SS-EK',\n",
       " 'EW_baseline_heuristic_ffill_SS-NK',\n",
       " 'NS_baseline_heuristic_ffill_ID-NK',\n",
       " 'NS_baseline_heuristic_ffill_IK-CK',\n",
       " 'NS_baseline_heuristic_ffill_IK-EK',\n",
       " 'NS_baseline_heuristic_ffill_SS-CK',\n",
       " 'NS_baseline_heuristic_ffill_SS-EK',\n",
       " 'NS_baseline_heuristic_ffill_SS-NK']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the training set: 450\n",
      "Number of objects in the validation set: 50\n"
     ]
    }
   ],
   "source": [
    "# Create a validation set without mixing the ObjectIDs\n",
    "object_ids = data['ObjectID'].unique()\n",
    "train_ids, valid_ids = train_test_split(object_ids, \n",
    "                                        test_size=config.valid_ratio, \n",
    "                                        random_state=43)\n",
    "\n",
    "train_data = data[data['ObjectID'].isin(train_ids)].copy()\n",
    "valid_data = data[data['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "ground_truth_train = ground_truth[ground_truth['ObjectID'].isin(train_ids)].copy()\n",
    "ground_truth_valid = ground_truth[ground_truth['ObjectID'].isin(valid_ids)].copy()\n",
    "\n",
    "# Count the number of objects in the training and validation sets\n",
    "print('Number of objects in the training set:', len(train_data['ObjectID'].unique()))\n",
    "print('Number of objects in the validation set:', len(valid_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make sure that there every label, both in the direction EW and NS,\n",
    "is present both in the training and validation partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of EW in test data: set()\n",
      "Missing values of NS in test data: set()\n",
      "Values of EW not present in NS: {'SS-EK', 'IK-EK', 'AD-NK'}\n"
     ]
    }
   ],
   "source": [
    "# Get the unique values of EW and NS in train and test data\n",
    "train_EW = set(train_data['EW'].unique())\n",
    "train_NS = set(train_data['NS'].unique())\n",
    "valid_EW = set(valid_data['EW'].unique())\n",
    "valid_NS = set(valid_data['NS'].unique())\n",
    "\n",
    "# Get the values of EW and NS that are in test data but not in train data\n",
    "missing_EW = valid_EW.difference(train_EW)\n",
    "missing_NS = valid_NS.difference(train_NS)\n",
    "\n",
    "# Check if all the values in EW are also present in NS\n",
    "if not set(train_data['EW'].unique()).issubset(set(train_data['NS'].unique())):\n",
    "    # Get the values of EW that are not present in NS\n",
    "    missing_EW_NS = set(train_data['EW'].unique()).difference(\n",
    "        set(train_data['NS'].unique())\n",
    "    )\n",
    "else:\n",
    "    missing_EW_NS = None\n",
    "\n",
    "# Print the missing values of EW and NS\n",
    "print(\"Missing values of EW in test data:\", missing_EW)\n",
    "print(\"Missing values of NS in test data:\", missing_NS)\n",
    "print(\"Values of EW not present in NS:\", missing_EW_NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert categorical data to numerical data\n",
    "le_EW = LabelEncoder()\n",
    "le_NS = LabelEncoder()\n",
    "\n",
    "# Encode the 'EW' and 'NS' columns\n",
    "train_data['EW_encoded'] = le_EW.fit_transform(train_data['EW'])\n",
    "train_data['NS_encoded'] = le_NS.fit_transform(train_data['NS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data['EW_baseline_heuristic_encoded'] = le_EW.transform(train_data['EW_baseline_heuristic'])\n",
    "# train_data['NS_baseline_heuristic_encoded'] = le_NS.transform(train_data['NS_baseline_heuristic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# valid_data['EW_baseline_heuristic_encoded'] = le_EW.transform(valid_data['EW_baseline_heuristic'])\n",
    "# valid_data['NS_baseline_heuristic_encoded'] = le_NS.transform(valid_data['NS_baseline_heuristic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.0671222\ttotal: 9.07s\tremaining: 14m 57s\n",
      "1:\tlearn: 0.0405626\ttotal: 17.1s\tremaining: 13m 56s\n",
      "2:\tlearn: 0.0258183\ttotal: 26.2s\tremaining: 14m 6s\n",
      "3:\tlearn: 0.0172030\ttotal: 34.4s\tremaining: 13m 45s\n",
      "4:\tlearn: 0.0115110\ttotal: 42.8s\tremaining: 13m 32s\n",
      "5:\tlearn: 0.0082843\ttotal: 51.2s\tremaining: 13m 21s\n",
      "6:\tlearn: 0.0062954\ttotal: 59.6s\tremaining: 13m 12s\n",
      "7:\tlearn: 0.0050997\ttotal: 1m 8s\tremaining: 13m 3s\n",
      "8:\tlearn: 0.0043049\ttotal: 1m 17s\tremaining: 12m 59s\n",
      "9:\tlearn: 0.0035480\ttotal: 1m 25s\tremaining: 12m 49s\n",
      "10:\tlearn: 0.0033006\ttotal: 1m 34s\tremaining: 12m 41s\n",
      "11:\tlearn: 0.0030791\ttotal: 1m 42s\tremaining: 12m 31s\n",
      "12:\tlearn: 0.0029849\ttotal: 1m 50s\tremaining: 12m 20s\n"
     ]
    }
   ],
   "source": [
    "# Define the Random Forest model for EW\n",
    "model_EW = CatBoostClassifier(n_estimators=100, random_state=42)\n",
    "# Fit the model to the training data for EW\n",
    "model_EW.fit(train_data[updated_feature_cols], train_data['EW_encoded'])\n",
    "\n",
    "# Define the Random Forest model for NS\n",
    "model_NS = CatBoostClassifier(n_estimators=100, random_state=42)\n",
    "# Fit the model to the training data for NS\n",
    "model_NS.fit(train_data[updated_feature_cols], train_data['NS_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_prediction(model,data,thresh):\n",
    "    pred_proba = pd.DataFrame(model.predict(data,prediction_type='Probability'))\n",
    "    pred = pred_proba.idxmax(1)\n",
    "    print('Num of ex to cut',sum(pred_proba.max(1)<thresh))\n",
    "    pred.loc[pred_proba.max(1)<thresh] = pred.value_counts().index[0]\n",
    "    pred = pred.to_numpy().reshape(-1,1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_ew = 0.1\n",
    "threshold_ns = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_EW.feature_importances_,index=model_EW.feature_names_).sort_values(0,ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_NS.feature_importances_,index=model_NS.feature_names_).sort_values(0,ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on the training data for EW\n",
    "train_data['Predicted_EW'] = le_EW.inverse_transform(\n",
    "    #model_EW.predict(train_data[updated_feature_cols])\n",
    "    do_prediction(model_EW,train_data[updated_feature_cols],threshold_ew)\n",
    ")\n",
    "\n",
    "# Make predictions on the validation data for NS\n",
    "train_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "    #model_NS.predict(train_data[updated_feature_cols])\n",
    "    do_prediction(model_NS,train_data[updated_feature_cols],threshold_ns)\n",
    ")\n",
    "\n",
    "train_data['Predicted_EW'] = train_data['Predicted_EW'].mask(train_data['Predicted_EW']=='Nothing').ffill()\n",
    "train_data['Predicted_NS'] = train_data['Predicted_NS'].mask(train_data['Predicted_NS']=='Nothing').ffill() \n",
    "\n",
    "# # Print the first few rows of the test data with predictions for both EW and NS\n",
    "# train_data[['TimeIndex', 'ObjectID', 'EW', \n",
    "#             'Predicted_EW', 'NS', 'Predicted_NS']].groupby('ObjectID').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_results = utils.convert_classifier_output(train_data)\n",
    "train_results.loc[train_results.TimeIndex==0,'Node'] = 'SS'\n",
    "evaluator = evaluation.NodeDetectionEvaluator(ground_truth_train, train_results, \n",
    "                                              tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the train set: {precision:.2f}')\n",
    "print(f'Recall for the train set: {recall:.2f}')\n",
    "print(f'F2 for the train set: {f2:.2f}')\n",
    "print(f'RMSE for the train set: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_ew = 0.1\n",
    "threshold_ns = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.valid_ratio > 0:\n",
    "    # Make predictions on the validation data for EW\n",
    "    valid_data['Predicted_EW'] = le_EW.inverse_transform(\n",
    "        #model_EW.predict(valid_data[updated_feature_cols])\n",
    "        do_prediction(model_EW,valid_data[updated_feature_cols],threshold_ew)\n",
    "    )\n",
    "\n",
    "    # Make predictions on the validation data for NS\n",
    "    valid_data['Predicted_NS'] = le_NS.inverse_transform(\n",
    "        #model_NS.predict(valid_data[updated_feature_cols])\n",
    "        do_prediction(model_NS,valid_data[updated_feature_cols],threshold_ns)\n",
    "    )\n",
    "    \n",
    "    valid_data['Predicted_EW'] = valid_data['Predicted_EW'].mask(valid_data['Predicted_EW']=='Nothing').ffill()\n",
    "    valid_data['Predicted_NS'] = valid_data['Predicted_NS'].mask(valid_data['Predicted_NS']=='Nothing').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NodeDetectionEvaluator` class in the evaluation module allows not only to\n",
    "compute the general score for a given dataset, but get evaluations per object, and\n",
    "even plots that show how the predictions look like in a timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.valid_ratio > 0:\n",
    "    valid_results = utils.convert_classifier_output(valid_data)\n",
    "    valid_results.loc[valid_results.TimeIndex==0,'Node'] = 'SS'\n",
    "\n",
    "    evaluator = evaluation.NodeDetectionEvaluator(ground_truth_valid, \n",
    "                                                  valid_results,\n",
    "                                                  tolerance=config.tolerance)\n",
    "precision, recall, f2, rmse = evaluator.score()\n",
    "print(f'Precision for the validation set: {precision:.2f}')\n",
    "print(f'Recall for the validation set: {recall:.2f}')\n",
    "print(f'F2 for the validation set: {f2:.2f}')\n",
    "print(f'RMSE for the validation set: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "#evaluator.plot(np.random.choice(train_data['ObjectID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over the Object IDs in the training set and call the evaluation\n",
    "# function for each object and aggregate the results\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "for oid in train_data['ObjectID'].unique():\n",
    "    tp, fp, fn, gt_object, p_object = evaluator.evaluate(oid)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "print(f'Total true positives: {total_tp}')\n",
    "print(f'Total false positives: {total_fp}')\n",
    "print(f'Total false negatives: {total_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation timeline for a random ObjectID from the training set\n",
    "evaluator.plot(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained random forest models (and label encoders) to disk\n",
    "# Create the folder trained_model if it doesn't exist\n",
    "Path('trained_model').mkdir(exist_ok=True)\n",
    "pickle.dump(model_EW, open('trained_model/model_EW_small.pkl', 'wb'))\n",
    "pickle.dump(model_NS, open('trained_model/model_NS_small.pkl', 'wb'))\n",
    "pickle.dump(le_EW, open('trained_model/le_EW_small.pkl', 'wb'))\n",
    "pickle.dump(le_NS, open('trained_model/le_NS_small.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
